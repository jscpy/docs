{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bienvenido a Mi Documentacion Gesu Dev For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Bienvenido a Mi Documentacion Gesu Dev"},{"location":"#bienvenido-a-mi-documentacion-gesu-dev","text":"For full documentation visit mkdocs.org .","title":"Bienvenido a Mi Documentacion Gesu Dev"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"about/","text":"Autumnus foedantem pulsa Noster iuncti si qui iussit vultu spumisque Lorem markdownum tenebras ubi quas cremata avia ululavit, deus miseris nocens Terram. Submisso moderante hunc . Par vivere expetitur digna litora choreas, pervenit vaccae, per Diamque ante vario, Iuno nam capillos. Petitam rubefecit celebravit Parcite gloria iugulatus ut nostro corpore , in quae nervosus? Qui et quamvis fiat quaeque et semina, sustinet ligones: animae vera vos aetherias manibus calidusque liquidas qui inter. pretest_technology_trackback(powerMulticastingLogic); if (20) { computerTeraflopsDefault.kerning_file_motion(virtualization, font_dial_qbe - gateway_ping); } else { clean.switch(onlineDns + metalVlogStorage); developmentViral.beta_dos(52, lamp, ipv + rpcSpam); } if (850656 - trim(pixel, 2, 5)) { soaRequirements(1, 5); overclockingAlu -= domainPointWhitelist(monitor + definitionCmykDslam, token); } else { parse = file; isdn.ultraManet /= null(transfer_arp_xml * bar); function(sequence_office_rdf); } Antro hoc utque superque functi tactas, tamen parentis vultusque currere felixque, bracchia, hic mutavit vulnera. Iuvenis Iuppiter corpore ante hoc. Aliqua dies conciderant transit tollens recepit mortalia convellere hoc clam o senecta. Esse nivea chlamydem stravit. Per opem nitentibus adhuc sit queritur quarum laniata parilique et armo, prorumpit se petita . versionProgressive.analyst_io = header_pci(359715 + daemon_tiff_online, duplex_thyristor); ata = internalIo.memory(format_gigaflops_forum + copyright, asp_wiki) + hdd( titleLeftMail); hyper_graphics_del(tokenMidi.codec(graphicMms, -3)); if (text(2 + 4) >= flash_vdsl_bezel(pmuCms, interlaced, buffer_ping_media)) { drive_insertion.winsock += trackbackPixel; } Vanam illi aer octonis lapides Iunoni puer pluma sinistra septem Iuppiter Thebae : deum Messeniaque urbem exspectanda pars recentia, animos Cinyras. Siqua aevumque effuge motisque, est peiora pondus ubi quoque sint reluxit me illuc et incerto perque penetratque virosque? Praepetis ruit nisi adversum consorte parvae est equi et tantum brevis. Lumen usum ebur imagine istis: caelo illo merito! Diro summo? Nunc vixi deicit sumus foedantem ad prima quater percussamque Mercurium vocavit ministro populusque quorum pineta tenuere suo . Non cur dum suum pedem nec aestus iuverat egerit; nec si Iole est femina. Queri videres, mihi mutet lingua ulmo Apollinei emergit. In siqua terra nec parentque tramite, nostro, oscula pectora, ab dederas. Ille frustra! Adde gemit et tandem , quis ut Thestiadae summo inbutam numina summissoque: nec. Tuli unda sacrarunt veluti sedebat, fuit, catenas annis; quo natae? Multorum Cereri pede egreditur cetera o Minervae mater et ales. Dente es simul Melampus, potitur hac: et per. Nullis constitit purpureas equi meruisse, est hos facto careat. Praecipuum dea iuga hoc, vir una pectore decimum in arbore structa lacrimasque Troes dolentem votisque Amphionis simul habuit, non. Prole auras Procne! Quo obruit damus saturae tori succedere, et inclitus datae retinentia? Aequoreae constat quod trahebat effugit rigent terga accipiunt dona decimum. Concita detruncatque vellet loquor luctantemque aptamque dixit: resolvit matre; quod acie, exire?","title":"Autumnus foedantem pulsa"},{"location":"about/#autumnus-foedantem-pulsa","text":"","title":"Autumnus foedantem pulsa"},{"location":"about/#noster-iuncti-si-qui-iussit-vultu-spumisque","text":"Lorem markdownum tenebras ubi quas cremata avia ululavit, deus miseris nocens Terram. Submisso moderante hunc . Par vivere expetitur digna litora choreas, pervenit vaccae, per Diamque ante vario, Iuno nam capillos. Petitam rubefecit celebravit Parcite gloria iugulatus ut nostro corpore , in quae nervosus? Qui et quamvis fiat quaeque et semina, sustinet ligones: animae vera vos aetherias manibus calidusque liquidas qui inter. pretest_technology_trackback(powerMulticastingLogic); if (20) { computerTeraflopsDefault.kerning_file_motion(virtualization, font_dial_qbe - gateway_ping); } else { clean.switch(onlineDns + metalVlogStorage); developmentViral.beta_dos(52, lamp, ipv + rpcSpam); } if (850656 - trim(pixel, 2, 5)) { soaRequirements(1, 5); overclockingAlu -= domainPointWhitelist(monitor + definitionCmykDslam, token); } else { parse = file; isdn.ultraManet /= null(transfer_arp_xml * bar); function(sequence_office_rdf); } Antro hoc utque superque functi tactas, tamen parentis vultusque currere felixque, bracchia, hic mutavit vulnera. Iuvenis Iuppiter corpore ante hoc. Aliqua dies conciderant transit tollens recepit mortalia convellere hoc clam o senecta. Esse nivea chlamydem stravit. Per opem nitentibus adhuc sit queritur quarum laniata parilique et armo, prorumpit se petita . versionProgressive.analyst_io = header_pci(359715 + daemon_tiff_online, duplex_thyristor); ata = internalIo.memory(format_gigaflops_forum + copyright, asp_wiki) + hdd( titleLeftMail); hyper_graphics_del(tokenMidi.codec(graphicMms, -3)); if (text(2 + 4) >= flash_vdsl_bezel(pmuCms, interlaced, buffer_ping_media)) { drive_insertion.winsock += trackbackPixel; }","title":"Noster iuncti si qui iussit vultu spumisque"},{"location":"about/#vanam-illi-aer-octonis-lapides","text":"Iunoni puer pluma sinistra septem Iuppiter Thebae : deum Messeniaque urbem exspectanda pars recentia, animos Cinyras. Siqua aevumque effuge motisque, est peiora pondus ubi quoque sint reluxit me illuc et incerto perque penetratque virosque? Praepetis ruit nisi adversum consorte parvae est equi et tantum brevis. Lumen usum ebur imagine istis: caelo illo merito! Diro summo? Nunc vixi deicit sumus foedantem ad prima quater percussamque Mercurium vocavit ministro populusque quorum pineta tenuere suo . Non cur dum suum pedem nec aestus iuverat egerit; nec si Iole est femina. Queri videres, mihi mutet lingua ulmo Apollinei emergit. In siqua terra nec parentque tramite, nostro, oscula pectora, ab dederas. Ille frustra! Adde gemit et tandem , quis ut Thestiadae summo inbutam numina summissoque: nec. Tuli unda sacrarunt veluti sedebat, fuit, catenas annis; quo natae? Multorum Cereri pede egreditur cetera o Minervae mater et ales. Dente es simul Melampus, potitur hac: et per. Nullis constitit purpureas equi meruisse, est hos facto careat. Praecipuum dea iuga hoc, vir una pectore decimum in arbore structa lacrimasque Troes dolentem votisque Amphionis simul habuit, non. Prole auras Procne! Quo obruit damus saturae tori succedere, et inclitus datae retinentia? Aequoreae constat quod trahebat effugit rigent terga accipiunt dona decimum. Concita detruncatque vellet loquor luctantemque aptamque dixit: resolvit matre; quod acie, exire?","title":"Vanam illi aer octonis lapides"},{"location":"influxdb/","text":"InfluxDB Installing telegraf and influxdb docker pull influxdb:1.7-alpine docker pull telegraf:1.11-alpine Networking docker network create some_network docker network create --driver bridge tick-net Run containers docker run -d --name influxdb --net=some_network -v /etc/influxdb/data/:/var/lib/influxdb -p 8083:8083 -p 8086:8086 influxdb:1.7-alpine docker run -d --name telegraf --net=container:influxdb telegraf:1.11-alpine Generate a config file docker run --rm telegraf:1.11-alpine telegraf config > telegraf.conf Using a custom telegraf config file docker run -d --name telegraf --net=some_network -v $PWD/telegraf.conf:/etc/telegraf/telegraf.conf:ro telegraf:1.11-alpine Gettings started on influxdb docker exec -it influxdb /bin/bash bash-4.4# influx -precision rfc3339 docker exec -it influxdb influx -precision rfc3339 > show databases; > use telegraf > show series > show MEASUREMENTS Authentication and authorization in InfluxDB > CREATE USER admin WITH PASSWORD '<password>' WITH ALL PRIVILEGES > SHOW USERS > REVOKE ALL PRIVILEGES FROM \"user\" # GRANT READ, WRITE or ALL database privileges to an existing user > GRANT [READ,WRITE,ALL] ON <database_name> TO <username> # DROP a user > DROP USER <username> Database management using InfluxQL # > CREATE DATABASE <database_name> [WITH [DURATION <duration>] [REPLICATION <n>] [SHARD DURATION <duration>] [NAME <retention-policy-name>]] > CREATE DATABASE \"NOAA_water_database\" WITH DURATION 3d REPLICATION 1 SHARD DURATION 1h NAME \"liquid\" > CREATE DATABASE \"dbname\" WITH DURATION 52w1d REPLICATION 1 SHARD DURATION 168h NAME \"dbname_rp_1_year\" > DROP DATABASE <database_name> # > DROP SERIES FROM <measurement_name[,measurement_name]> WHERE <tag_key>='<tag_value>' > DROP SERIES WHERE \"location\" = 'santa_monica' # > DELETE FROM <measurement_name> WHERE [<tag_key>='<tag_value>'] | [<time interval>] > DELETE FROM \"h2o_feet\" Retention policy management > SHOW RETENTION POLICIES > CREATE RETENTION POLICY <retention_policy_name> ON <database_name> DURATION <duration> REPLICATION <n> [SHARD DURATION <duration>] [DEFAULT] > CREATE RETENTION POLICY \"one_day_only\" ON \"NOAA_water_database\" DURATION 1d REPLICATION 1 # Modify retention policies with ALTER RETENTION POLICY > ALTER RETENTION POLICY <retention_policy_name> ON <database_name> DURATION <duration> REPLICATION <n> SHARD DURATION <duration> DEFAULT > CREATE RETENTION POLICY \"what_is_time\" ON \"NOAA_water_database\" DURATION 2d REPLICATION 1 > ALTER RETENTION POLICY \"what_is_time\" ON \"NOAA_water_database\" DURATION 3w SHARD DURATION 2h DEFAULT # Delete retention policies with DROP RETENTION POLICY > DROP RETENTION POLICY <retention_policy_name> ON <database_name> > DROP RETENTION POLICY \"what_is_time\" ON \"NOAA_water_database\" DATABASE LABORATORIO IG > CREATE DATABASE \"noc\" WITH DURATION 52w1d REPLICATION 1 SHARD DURATION 168h NAME \"noc_1_year\" > CREATE USER monitoreo WITH PASSWORD 'monitoreo2019.*' > CREATE USER dashboard WITH PASSWORD 'dashboard2020.*' > GRANT READ ON \"noc\" TO \"dashboard\" > GRANT WRITE ON \"noc\" TO \"monitoreo\" > SHOW USERS > SHOW GRANTS FOR <user_name>","title":"Influxdb"},{"location":"influxdb/#influxdb","text":"","title":"InfluxDB"},{"location":"influxdb/#installing-telegraf-and-influxdb","text":"docker pull influxdb:1.7-alpine docker pull telegraf:1.11-alpine","title":"Installing telegraf and influxdb"},{"location":"influxdb/#networking","text":"docker network create some_network docker network create --driver bridge tick-net","title":"Networking"},{"location":"influxdb/#run-containers","text":"docker run -d --name influxdb --net=some_network -v /etc/influxdb/data/:/var/lib/influxdb -p 8083:8083 -p 8086:8086 influxdb:1.7-alpine docker run -d --name telegraf --net=container:influxdb telegraf:1.11-alpine","title":"Run containers"},{"location":"influxdb/#generate-a-config-file","text":"docker run --rm telegraf:1.11-alpine telegraf config > telegraf.conf","title":"Generate a config file"},{"location":"influxdb/#using-a-custom-telegraf-config-file","text":"docker run -d --name telegraf --net=some_network -v $PWD/telegraf.conf:/etc/telegraf/telegraf.conf:ro telegraf:1.11-alpine","title":"Using a custom telegraf config file"},{"location":"influxdb/#gettings-started-on-influxdb","text":"docker exec -it influxdb /bin/bash bash-4.4# influx -precision rfc3339 docker exec -it influxdb influx -precision rfc3339 > show databases; > use telegraf > show series > show MEASUREMENTS","title":"Gettings started on influxdb"},{"location":"influxdb/#authentication-and-authorization-in-influxdb","text":"> CREATE USER admin WITH PASSWORD '<password>' WITH ALL PRIVILEGES > SHOW USERS > REVOKE ALL PRIVILEGES FROM \"user\" # GRANT READ, WRITE or ALL database privileges to an existing user > GRANT [READ,WRITE,ALL] ON <database_name> TO <username> # DROP a user > DROP USER <username>","title":"Authentication and authorization in InfluxDB"},{"location":"influxdb/#database-management-using-influxql","text":"# > CREATE DATABASE <database_name> [WITH [DURATION <duration>] [REPLICATION <n>] [SHARD DURATION <duration>] [NAME <retention-policy-name>]] > CREATE DATABASE \"NOAA_water_database\" WITH DURATION 3d REPLICATION 1 SHARD DURATION 1h NAME \"liquid\" > CREATE DATABASE \"dbname\" WITH DURATION 52w1d REPLICATION 1 SHARD DURATION 168h NAME \"dbname_rp_1_year\" > DROP DATABASE <database_name> # > DROP SERIES FROM <measurement_name[,measurement_name]> WHERE <tag_key>='<tag_value>' > DROP SERIES WHERE \"location\" = 'santa_monica' # > DELETE FROM <measurement_name> WHERE [<tag_key>='<tag_value>'] | [<time interval>] > DELETE FROM \"h2o_feet\"","title":"Database management using InfluxQL"},{"location":"influxdb/#retention-policy-management","text":"> SHOW RETENTION POLICIES > CREATE RETENTION POLICY <retention_policy_name> ON <database_name> DURATION <duration> REPLICATION <n> [SHARD DURATION <duration>] [DEFAULT] > CREATE RETENTION POLICY \"one_day_only\" ON \"NOAA_water_database\" DURATION 1d REPLICATION 1 # Modify retention policies with ALTER RETENTION POLICY > ALTER RETENTION POLICY <retention_policy_name> ON <database_name> DURATION <duration> REPLICATION <n> SHARD DURATION <duration> DEFAULT > CREATE RETENTION POLICY \"what_is_time\" ON \"NOAA_water_database\" DURATION 2d REPLICATION 1 > ALTER RETENTION POLICY \"what_is_time\" ON \"NOAA_water_database\" DURATION 3w SHARD DURATION 2h DEFAULT # Delete retention policies with DROP RETENTION POLICY > DROP RETENTION POLICY <retention_policy_name> ON <database_name> > DROP RETENTION POLICY \"what_is_time\" ON \"NOAA_water_database\"","title":"Retention policy management"},{"location":"influxdb/#database-laboratorio-ig","text":"> CREATE DATABASE \"noc\" WITH DURATION 52w1d REPLICATION 1 SHARD DURATION 168h NAME \"noc_1_year\" > CREATE USER monitoreo WITH PASSWORD 'monitoreo2019.*' > CREATE USER dashboard WITH PASSWORD 'dashboard2020.*' > GRANT READ ON \"noc\" TO \"dashboard\" > GRANT WRITE ON \"noc\" TO \"monitoreo\" > SHOW USERS > SHOW GRANTS FOR <user_name>","title":"DATABASE LABORATORIO IG"},{"location":"javascript/","text":"Moder JavaScript DOM Selector for a Single Element document.getElementById('elementId: DOMString') // Get things from the element document.getElementById('elementId: DOMString').id document.getElementById('elementId: DOMString').className // Change styling document.getElementById('elementId: DOMString').style.background = '#3333' document.getElementById('elementId: DOMString').style.color = '#3333' document.getElementById('elementId: DOMString').style.display = 'none' // Change content document.getElementById('elementId: DOMString').textContent = '' document.getElementById('elementId: DOMString').innerText = '' document.getElementById('elementId: DOMString').innerHTML = '<span></span>' document.querySelector('selectors: DOMString') document.querySelector('li').style.color = 'red'; document.querySelector('li:last-child').textContent = 'Hello' document.querySelector('li:ntn-child(3').style.background = '#ccc'; document.querySelector('li:ntn-child(odd)').textContent = '#FFF' document.querySelector('li:ntn-child(even').textContent = '#000' DOM Selectors for Multiple elements document.getElementsByClassName('classNames: DOMString') const items = document.getElementsByClassName('items') console.log(items) console.log(items[0]) items[0].style.color = 'red' items[0].textContent = 'Hello' const listItems = document.querySelector('ul').getElementsByClassName('collection') document.getElementsByTagName('localName: DOMString') const lis = document.getElementsByTagName('li'); console.log(lis) console.log(lis[0]) lis[0].style.color = 'red' lis[0].textContent = 'Hello' lis = Array.from(lis) lis.forEach(function(li, index) { console.log(li.className) li.textContent = `${index}: Hello`; }); document.querySelectorAll('selectors: DOMString') // NodeList const items = document.querySelectorAll('ul.collection li.collection-item') items.forEach( function(item, index) { item.textContent = `${index}: Hello`; }); const liOdd = document.querySelectorAll('li:nth-child(odd'); const liEven = document.querySelectorAll('li:nth-child(even'); liOdd.forEach( function(li, index) { li.style.background = '#ccc'; }); for(let i = 0; i < liEven.length; i++){ liEven[i].style.background = '#F4F4F4'; } const list = document.querySelector('ul.collection') list.children val = list.childNodes[0]; val = list.childNodes[0].nodeName; val = list.childNodes[0].nodeType; // 1 - element // 2 - Attribute // 3 - Text node // 8 - Comment // 9 - Document itself // 10 - Doctype Creating elemets const li = document.createElement('li') li.className = 'collection-item' li.id = 'new-item' li.setAttribute('title', 'New Item') li.appendChild(document.createTextNode('Hello World')) const link = document.createElement('a') link.className = 'delete item' link.innerHTML = '<i class=\"fa fa-remove\"></i>' li.appendChild(link) document.querySelector('ul.collecion').appendChild(li) Event listeners & Event object document.querySelector('.clear-tasksk').addEventListener('click', onClick); function onClick(e){ e.preventDefault() val = e // Event target element val = e.target val = e.target.id val = e.target.className val = e.target.classList e.target.innerText = 'Hello' }","title":"Moder JavaScript"},{"location":"javascript/#moder-javascript","text":"","title":"Moder JavaScript"},{"location":"javascript/#dom-selector-for-a-single-element","text":"document.getElementById('elementId: DOMString') // Get things from the element document.getElementById('elementId: DOMString').id document.getElementById('elementId: DOMString').className // Change styling document.getElementById('elementId: DOMString').style.background = '#3333' document.getElementById('elementId: DOMString').style.color = '#3333' document.getElementById('elementId: DOMString').style.display = 'none' // Change content document.getElementById('elementId: DOMString').textContent = '' document.getElementById('elementId: DOMString').innerText = '' document.getElementById('elementId: DOMString').innerHTML = '<span></span>' document.querySelector('selectors: DOMString') document.querySelector('li').style.color = 'red'; document.querySelector('li:last-child').textContent = 'Hello' document.querySelector('li:ntn-child(3').style.background = '#ccc'; document.querySelector('li:ntn-child(odd)').textContent = '#FFF' document.querySelector('li:ntn-child(even').textContent = '#000'","title":"DOM Selector for a Single Element"},{"location":"javascript/#dom-selectors-for-multiple-elements","text":"document.getElementsByClassName('classNames: DOMString') const items = document.getElementsByClassName('items') console.log(items) console.log(items[0]) items[0].style.color = 'red' items[0].textContent = 'Hello' const listItems = document.querySelector('ul').getElementsByClassName('collection') document.getElementsByTagName('localName: DOMString') const lis = document.getElementsByTagName('li'); console.log(lis) console.log(lis[0]) lis[0].style.color = 'red' lis[0].textContent = 'Hello' lis = Array.from(lis) lis.forEach(function(li, index) { console.log(li.className) li.textContent = `${index}: Hello`; }); document.querySelectorAll('selectors: DOMString') // NodeList const items = document.querySelectorAll('ul.collection li.collection-item') items.forEach( function(item, index) { item.textContent = `${index}: Hello`; }); const liOdd = document.querySelectorAll('li:nth-child(odd'); const liEven = document.querySelectorAll('li:nth-child(even'); liOdd.forEach( function(li, index) { li.style.background = '#ccc'; }); for(let i = 0; i < liEven.length; i++){ liEven[i].style.background = '#F4F4F4'; } const list = document.querySelector('ul.collection') list.children val = list.childNodes[0]; val = list.childNodes[0].nodeName; val = list.childNodes[0].nodeType; // 1 - element // 2 - Attribute // 3 - Text node // 8 - Comment // 9 - Document itself // 10 - Doctype","title":"DOM Selectors for Multiple elements"},{"location":"javascript/#creating-elemets","text":"const li = document.createElement('li') li.className = 'collection-item' li.id = 'new-item' li.setAttribute('title', 'New Item') li.appendChild(document.createTextNode('Hello World')) const link = document.createElement('a') link.className = 'delete item' link.innerHTML = '<i class=\"fa fa-remove\"></i>' li.appendChild(link) document.querySelector('ul.collecion').appendChild(li)","title":"Creating elemets"},{"location":"javascript/#event-listeners-event-object","text":"document.querySelector('.clear-tasksk').addEventListener('click', onClick); function onClick(e){ e.preventDefault() val = e // Event target element val = e.target val = e.target.id val = e.target.className val = e.target.classList e.target.innerText = 'Hello' }","title":"Event listeners &amp; Event object"},{"location":"prometheus/","text":"Prometheus snippets Prometheus Power your metrics and alerting with a leading open-source monitoring solution. Configuraci\u00f3n B\u00e1sica Prometheus es un archivo binario y no necesita dependecias externas. La configuraci\u00f3n se realiza creando un archivo YAML nombrado prometheus.yml . ./prometheus --config.file=prometheus.yml --web.listen-address=\"0.0.0.0:9090\" # my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\" # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] Configuraci\u00f3n Avanzada File-Based Service Discovery La configuraci\u00f3n de Prometheus cambia dependiedo de los exporters utilizados. Si queremos definir de manera din\u00e1mica los targets que ser\u00e1n monitoreados podemos hacerlo usando un JSON. Dentro de este archivo estara la lista de hosts junto con las etiquetas que serviran para identifcarlos y agruparlos. [ { \"targets\": [ \"4.4.4.4\" ], \"labels\": { \"check\": \"icmp\", \"service\": \"servicio-001\" } }, { \"targets\": [ \"127.0.0.1\"], \"labels\": { \"check\": \"snmp\", \"service\": \"servicio-002\" } } ] Asumiendo que estamos utilizando blackbox_exporter y snmp_exporter , la configuraci\u00f3n recomendada seria la siguiente: global: scrape_interval: 1m scrape_configs: - job_name: 'blackbox' metrics_path: /probe params: module: [icmp_ipv4] file_sd_configs: - files: - 'targets.json' relabel_configs: - source_labels: [\"check\"] regex: (icmp|all) action: keep - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9115 # This is your blackbox exporter. metric_relabel_configs: - source_labels: [__name__] regex: go_(.*) action: drop - source_labels: [__name__] regex: pushgateway_(.*) action: drop - source_labels: [__name__] regex: promhttp_(.*) action: drop - source_labels: [__name__] regex: process_(.*) action: drop - job_name: 'snmp' file_sd_configs: - files: - 'targets.json' metrics_path: /snmp params: module: [if_mib] relabel_configs: - source_labels: [\"check\"] regex: (snmp|all) action: keep - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9116 # The SNMP exporter's real hostname:port. metric_relabel_configs: - source_labels: [__name__] regex: go_(.*) action: drop - source_labels: [__name__] regex: pushgateway_(.*) action: drop - source_labels: [__name__] regex: promhttp_(.*) action: drop - source_labels: [__name__] regex: process_(.*) action: drop remote_write: - url: \"http://remote-server-ip:9201/write\" remote_read: - url: \"http://remote-server-ip:9201/read\" Esta configuraci\u00f3n nos ofrece la siguientes ventajas: Podemos separa los targets por tipo de checks que queramos realizar ( ICMP y SNMP en este caso). Si queremos agregar un nuevo target , basta con agregarlo como un nuevo objecto al archivo JSON. En metric_relabel_configs , usando expresiones regulares filtramos las metricas que no necesitamos y no seran guardadas en la base de datos. Consul Service Discovery Clic aqu\u00ed para ver m\u00e1s detalles de Consul. La integraci\u00f3n con Prometheus se realiza con los siguientes cambios en el archivo prometheus.yml global: scrape_interval: 1m scrape_configs: - job_name: 'blackbox' metrics_path: /probe params: module: [icmp_ipv4] consul_sd_configs: - server: 'localhost:8500' relabel_configs: - source_labels: [__meta_consul_service] target_label: service - source_labels: ['__address__'] target_label: '__param_target' regex: '([^:]+)(:.*)?' replacement: '$1' - source_labels: [__meta_consul_service_metadata_check] regex: (icmp|all) action: keep - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9115 - job_name: 'snmp' metrics_path: /snmp params: module: [if_mib] consul_sd_configs: - server: '127.0.0.1:8500' relabel_configs: - source_labels: [__meta_consul_service] target_label: service - source_labels: ['__address__'] target_label: '__param_target' regex: '([^:]+)(:.*)?' replacement: '$1' - source_labels: [__meta_consul_service_metadata_check] regex: (snmp|all) action: keep - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9116 En relabel_configs a\u00f1adimos una nueva fuente de informaci\u00f3n que ser\u00e1n los servicios definidos en el API Rest de Consul y que por cada nuevo label podemos leer cada uno de ellos desde Prometheus a\u00f1adiendo un nuevo source_labels por cada etiqueta. - source_labels: [__meta_consul_service_metadata_label] regex: (regex) action: keep | drop SNMP Exporter SNMP exporter exposes information gathered from SNMP ( by port :9116 ) for use by the Prometheus monitoring system. ./snmp_exporter SNMP exporter lee la definici\u00f3n de MIBS generados en el archivo snmp.yml . Si el protocolo tiene una comunidad definida, podemos agregarlo dentro de auth de la siguiente forma. if_mib: version: 2 auth: community: 1nteligl0b3 walk: - 1.3.6.1.2.1.2 - 1.3.6.1.2.1.31.1.1 get: - 1.3.6.1.2.1.1.3.0 metrics: - name: sysUpTime oid: 1.3.6.1.2.1.1.3 type: gauge help: The time (in hundredths of a second) since the network management portion of the system was last re-initialized. - 1.3.6.1.2.1.1.3 - name: ifNumber oid: 1.3.6.1.2.1.2.1 type: gauge help: The number of network interfaces (regardless of their current state) present on this system. - 1.3.6.1.2.1.2.1 - name: ifIndex oid: 1.3.6.1.2.1.2.2.1.1 type: gauge help: A unique value, greater than zero, for each interface - 1.3.6.1.2.1.2.2.1.1 indexes: - labelname: ifIndex type: gauge - name: ifDescr Blackbox Exporter The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP and ICMP. Exposes information by port :9115 ./blackbox_exporter La definici\u00f3n de los modulos de Blackbox se hacen en el archivo blackbox.yml modules: icmp_ipv4: prober: icmp icmp: preferred_ip_protocol: ip4 Consul Consul proporciona un plano de control completo con funciones de descubrimiento, configuraci\u00f3n y segmentaci\u00f3n de servicios. Podeos definir la configuraci\u00f3n ( config.json ) de Consul dentro de un direcotrio config.d y definirlo de la siguiente manera. { \"client_addr\": \"0.0.0.0\", \"datacenter\": \"\", \"node_name\": \"\", \"data_dir\": \"/tmp/consul\", \"encrypt\": \"[consul_keygen]\", \"log_level\": \"INFO\" } ./consul keygen ./consul agent -dev -config-dir=config.d Alert Manager Alerting with Prometheus is separated into two parts: Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, on-call notification systems, and chat platforms. ./alertmanager --config.file=alertmanager.yml Toda la configuraci\u00f3n de Alert manager se define en el archivo alertmanager.yml global: resolve_timeout: 5m smtp_smarthost: 'smtp.inteliglobe.com:587' smtp_from: 'jesolis@inteliglobe.com' smtp_auth_username: 'jesolis' smtp_auth_password: '123456789a@@' smtp_require_tls: false route: group_by: ['alertname'] group_wait: 1m group_interval: 1m repeat_interval: 1h receiver: 'email-to-soporte' receivers: - name: 'email-to-soporte' email_configs: - to: 'jsc.py.14@gmail.com' inhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] En Prometheus la configuraci\u00f3n en prometheus.yml debe contener lo siguiente: global: scrape_interval: 1m rule_files: - \"alert.rules.yml\" # ... alerting: alertmanagers: - scheme: http static_configs: - targets: - \"alertmanager:9093\" alert.rules.yml nos permite definir las reglas de alerta Regla para determinar si el disco llego a 80% groups: - name: aws-filesystem rules: - alert: aws_file_system_full expr: 100-((node_filesystem_free_bytes{mountpoint=\"/home\"}/node_filesystem_size_bytes{mountpoint=\"/home\"})*100) > 20 for: 5m annotations: summary: \"La instancia de amazon no tiene espacio en disco\" description: \"La instancia de amazon no tiene espacio en disco\" Reglas para definir si un host o servicio esta fuera de l\u00ednea. - alert: InstanceDown expr: instance:probe_success == 0 for: 5m annotations: summary: \"Instance {{ $labels.instance }} down\" description: \"{{ $labels.instance }} has been down for more than 5 minutes.\" - alert: ServiceDown expr: instance:ifOperStatus == 2 for: 5m annotations: summary: \"Service of {{ $labels.instance }} down\" description: \"Service {{ $labels.instance }} has been down for more than 5 minutes.\" Federation Federation allows a Prometheus server to scrape selected time series from another Prometheus server. There are different use cases for federation. Commonly, it is used to either achieve scalable Prometheus monitoring setups or to pull related metrics from one service's Prometheus into another. # slave prometheus.yml file global: external_labels: slave: slave-name-or-another-tag # master prometheus.yml file scrape_configs: - job_name: 'federate' scrape_interval: 1m honor_labels: true metrics_path: '/federate' params: 'match[]': # - '{__name__=~\"job:.*\"}' - '{job=\"blackbox\"}' - '{job=\"snmp\"}' static_configs: - targets: - 'prometheus_01:9090' - 'prometheus_02:9100' - 'prometheus_03:9110' # ... Python Client The official Python 2 and 3 client for Prometheus. pip install prometheus_client See more in Python client github repository.","title":"Prometheus"},{"location":"prometheus/#prometheus-snippets","text":"","title":"Prometheus snippets"},{"location":"prometheus/#prometheus","text":"Power your metrics and alerting with a leading open-source monitoring solution.","title":"Prometheus"},{"location":"prometheus/#configuracion-basica","text":"Prometheus es un archivo binario y no necesita dependecias externas. La configuraci\u00f3n se realiza creando un archivo YAML nombrado prometheus.yml . ./prometheus --config.file=prometheus.yml --web.listen-address=\"0.0.0.0:9090\" # my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\" # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090']","title":"Configuraci\u00f3n B\u00e1sica"},{"location":"prometheus/#configuracion-avanzada","text":"","title":"Configuraci\u00f3n Avanzada"},{"location":"prometheus/#file-based-service-discovery","text":"La configuraci\u00f3n de Prometheus cambia dependiedo de los exporters utilizados. Si queremos definir de manera din\u00e1mica los targets que ser\u00e1n monitoreados podemos hacerlo usando un JSON. Dentro de este archivo estara la lista de hosts junto con las etiquetas que serviran para identifcarlos y agruparlos. [ { \"targets\": [ \"4.4.4.4\" ], \"labels\": { \"check\": \"icmp\", \"service\": \"servicio-001\" } }, { \"targets\": [ \"127.0.0.1\"], \"labels\": { \"check\": \"snmp\", \"service\": \"servicio-002\" } } ] Asumiendo que estamos utilizando blackbox_exporter y snmp_exporter , la configuraci\u00f3n recomendada seria la siguiente: global: scrape_interval: 1m scrape_configs: - job_name: 'blackbox' metrics_path: /probe params: module: [icmp_ipv4] file_sd_configs: - files: - 'targets.json' relabel_configs: - source_labels: [\"check\"] regex: (icmp|all) action: keep - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9115 # This is your blackbox exporter. metric_relabel_configs: - source_labels: [__name__] regex: go_(.*) action: drop - source_labels: [__name__] regex: pushgateway_(.*) action: drop - source_labels: [__name__] regex: promhttp_(.*) action: drop - source_labels: [__name__] regex: process_(.*) action: drop - job_name: 'snmp' file_sd_configs: - files: - 'targets.json' metrics_path: /snmp params: module: [if_mib] relabel_configs: - source_labels: [\"check\"] regex: (snmp|all) action: keep - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9116 # The SNMP exporter's real hostname:port. metric_relabel_configs: - source_labels: [__name__] regex: go_(.*) action: drop - source_labels: [__name__] regex: pushgateway_(.*) action: drop - source_labels: [__name__] regex: promhttp_(.*) action: drop - source_labels: [__name__] regex: process_(.*) action: drop remote_write: - url: \"http://remote-server-ip:9201/write\" remote_read: - url: \"http://remote-server-ip:9201/read\" Esta configuraci\u00f3n nos ofrece la siguientes ventajas: Podemos separa los targets por tipo de checks que queramos realizar ( ICMP y SNMP en este caso). Si queremos agregar un nuevo target , basta con agregarlo como un nuevo objecto al archivo JSON. En metric_relabel_configs , usando expresiones regulares filtramos las metricas que no necesitamos y no seran guardadas en la base de datos.","title":"File-Based Service Discovery"},{"location":"prometheus/#consul-service-discovery","text":"Clic aqu\u00ed para ver m\u00e1s detalles de Consul. La integraci\u00f3n con Prometheus se realiza con los siguientes cambios en el archivo prometheus.yml global: scrape_interval: 1m scrape_configs: - job_name: 'blackbox' metrics_path: /probe params: module: [icmp_ipv4] consul_sd_configs: - server: 'localhost:8500' relabel_configs: - source_labels: [__meta_consul_service] target_label: service - source_labels: ['__address__'] target_label: '__param_target' regex: '([^:]+)(:.*)?' replacement: '$1' - source_labels: [__meta_consul_service_metadata_check] regex: (icmp|all) action: keep - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9115 - job_name: 'snmp' metrics_path: /snmp params: module: [if_mib] consul_sd_configs: - server: '127.0.0.1:8500' relabel_configs: - source_labels: [__meta_consul_service] target_label: service - source_labels: ['__address__'] target_label: '__param_target' regex: '([^:]+)(:.*)?' replacement: '$1' - source_labels: [__meta_consul_service_metadata_check] regex: (snmp|all) action: keep - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9116 En relabel_configs a\u00f1adimos una nueva fuente de informaci\u00f3n que ser\u00e1n los servicios definidos en el API Rest de Consul y que por cada nuevo label podemos leer cada uno de ellos desde Prometheus a\u00f1adiendo un nuevo source_labels por cada etiqueta. - source_labels: [__meta_consul_service_metadata_label] regex: (regex) action: keep | drop","title":"Consul Service Discovery"},{"location":"prometheus/#snmp-exporter","text":"SNMP exporter exposes information gathered from SNMP ( by port :9116 ) for use by the Prometheus monitoring system. ./snmp_exporter SNMP exporter lee la definici\u00f3n de MIBS generados en el archivo snmp.yml . Si el protocolo tiene una comunidad definida, podemos agregarlo dentro de auth de la siguiente forma. if_mib: version: 2 auth: community: 1nteligl0b3 walk: - 1.3.6.1.2.1.2 - 1.3.6.1.2.1.31.1.1 get: - 1.3.6.1.2.1.1.3.0 metrics: - name: sysUpTime oid: 1.3.6.1.2.1.1.3 type: gauge help: The time (in hundredths of a second) since the network management portion of the system was last re-initialized. - 1.3.6.1.2.1.1.3 - name: ifNumber oid: 1.3.6.1.2.1.2.1 type: gauge help: The number of network interfaces (regardless of their current state) present on this system. - 1.3.6.1.2.1.2.1 - name: ifIndex oid: 1.3.6.1.2.1.2.2.1.1 type: gauge help: A unique value, greater than zero, for each interface - 1.3.6.1.2.1.2.2.1.1 indexes: - labelname: ifIndex type: gauge - name: ifDescr","title":"SNMP Exporter"},{"location":"prometheus/#blackbox-exporter","text":"The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP and ICMP. Exposes information by port :9115 ./blackbox_exporter La definici\u00f3n de los modulos de Blackbox se hacen en el archivo blackbox.yml modules: icmp_ipv4: prober: icmp icmp: preferred_ip_protocol: ip4","title":"Blackbox Exporter"},{"location":"prometheus/#consul","text":"Consul proporciona un plano de control completo con funciones de descubrimiento, configuraci\u00f3n y segmentaci\u00f3n de servicios. Podeos definir la configuraci\u00f3n ( config.json ) de Consul dentro de un direcotrio config.d y definirlo de la siguiente manera. { \"client_addr\": \"0.0.0.0\", \"datacenter\": \"\", \"node_name\": \"\", \"data_dir\": \"/tmp/consul\", \"encrypt\": \"[consul_keygen]\", \"log_level\": \"INFO\" } ./consul keygen ./consul agent -dev -config-dir=config.d","title":"Consul"},{"location":"prometheus/#alert-manager","text":"Alerting with Prometheus is separated into two parts: Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, on-call notification systems, and chat platforms. ./alertmanager --config.file=alertmanager.yml Toda la configuraci\u00f3n de Alert manager se define en el archivo alertmanager.yml global: resolve_timeout: 5m smtp_smarthost: 'smtp.inteliglobe.com:587' smtp_from: 'jesolis@inteliglobe.com' smtp_auth_username: 'jesolis' smtp_auth_password: '123456789a@@' smtp_require_tls: false route: group_by: ['alertname'] group_wait: 1m group_interval: 1m repeat_interval: 1h receiver: 'email-to-soporte' receivers: - name: 'email-to-soporte' email_configs: - to: 'jsc.py.14@gmail.com' inhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] En Prometheus la configuraci\u00f3n en prometheus.yml debe contener lo siguiente: global: scrape_interval: 1m rule_files: - \"alert.rules.yml\" # ... alerting: alertmanagers: - scheme: http static_configs: - targets: - \"alertmanager:9093\" alert.rules.yml nos permite definir las reglas de alerta Regla para determinar si el disco llego a 80% groups: - name: aws-filesystem rules: - alert: aws_file_system_full expr: 100-((node_filesystem_free_bytes{mountpoint=\"/home\"}/node_filesystem_size_bytes{mountpoint=\"/home\"})*100) > 20 for: 5m annotations: summary: \"La instancia de amazon no tiene espacio en disco\" description: \"La instancia de amazon no tiene espacio en disco\" Reglas para definir si un host o servicio esta fuera de l\u00ednea. - alert: InstanceDown expr: instance:probe_success == 0 for: 5m annotations: summary: \"Instance {{ $labels.instance }} down\" description: \"{{ $labels.instance }} has been down for more than 5 minutes.\" - alert: ServiceDown expr: instance:ifOperStatus == 2 for: 5m annotations: summary: \"Service of {{ $labels.instance }} down\" description: \"Service {{ $labels.instance }} has been down for more than 5 minutes.\"","title":"Alert Manager"},{"location":"prometheus/#federation","text":"Federation allows a Prometheus server to scrape selected time series from another Prometheus server. There are different use cases for federation. Commonly, it is used to either achieve scalable Prometheus monitoring setups or to pull related metrics from one service's Prometheus into another. # slave prometheus.yml file global: external_labels: slave: slave-name-or-another-tag # master prometheus.yml file scrape_configs: - job_name: 'federate' scrape_interval: 1m honor_labels: true metrics_path: '/federate' params: 'match[]': # - '{__name__=~\"job:.*\"}' - '{job=\"blackbox\"}' - '{job=\"snmp\"}' static_configs: - targets: - 'prometheus_01:9090' - 'prometheus_02:9100' - 'prometheus_03:9110' # ...","title":"Federation"},{"location":"prometheus/#python-client","text":"The official Python 2 and 3 client for Prometheus. pip install prometheus_client See more in Python client github repository.","title":"Python Client"},{"location":"python/","text":"Python Tips & Chips Run a function at the start of every round 5 minute interval from datetime import datetime, timedelta from threading import Timer def get_sleep_time(): now = datetime.now() next_run = now.replace(minute=int(now.minute / 5) * 5, second=0, microsecond=0) + timedelta(minutes=5) return (next_run - now).total_seconds() def dowork(): now = datetime.now() print('Doing some work at', now) schedule_next_run() def schedule_next_run(): sleep_time = get_sleep_time() print(f'sleeping for {sleep_time} seconds') t = Timer(sleep_time, dowork) t.daemon = True t.start() print('Starting work schedule') schedule_next_run() input('Doing work every 5 minutes. Press enter to exit') Managing Multiple Python Versions With Pyenv Performance Tips Good logging practice in Python 30 Essential Python Tips and Tricks for Programmers 30 Python Language Features and Tricks You May Not Know About 9 obscure Python libraries for data science Speed Up Your Python Program With Concurrency The ever useful and neat subprocess module Production-ready Docker packaging Easy parallel HTTP requests with Python and asyncio IP Geolocation API 13 Project Ideas for Intermediate Python Developers A Simple Approach To Templated SQL Queries In Python Twitter Sentiment Analysis Twitter Python Developing and Testing an Asynchronous API with FastAPI and Pytest fbs tutorial Get only NEW Emails imaplib and python Web Django Django REST with React Django Dashboard CoreUI Flask Flask Dashboard - CoreUI Design GUI PySide2 Qt for Python Documentation Qt for Python Tutorials PySide2 examples Pyqtgraph Building a Matplotlib GUI with Qt Designer How to Install PyQt5 and Build Your First GUI in Python 3.4 Kivy Kivy VideoPlayer not working: Error loading Texture #6672 pip install pillow ffpyplayer How To Make Simple Mobile Applications Using Kivy In Python? Netclone Netflix UI Clone Audio and Video Streaming issue on Android Databases Writing Data to InfluxDB with Python Playing with Postgres NOTIFY/LISTEN using Python asyncio and psycopg2 Fastest Way to Load Data Into PostgreSQL Using Python Async Tasks Huey Huey - a nice celery alternative Celery With Flask from flask import Flask from celery import Celery app = Flask(__name__) app.config['CELERY_BROKER_URL'] = 'redis://localhost:6379' app.config['CELERY_RESULT_BACKEND'] = 'redis://localhost:6379' celery = Celery(app.name, broker=app.config['CELERY_BROKER_URL']) celery.conf.update(app.config) @celery.task() def some_task(): # ... return {\"result\": \"success\"} @app.route('/some-route') def encoder(): task = some_task.delay() # ... return {\"task_id\": task.id} if __name__ == \"__main__\": app.run(debug=True) Django & Celery \u2013 Easy async task processing Asynchronous Tasks using Celery with Django Setting up an asynchronous task queue for Django using Celery and Redis Asynchronous Tasks With Django and Celery Don't keep important data in your Celery queue How to keep Celery running with supervisor Storing a task id for each celery task in database Celery Exception Handling Running Asynchronous background Tasks on Linux with Python 3 Flask and Celery Cient\u00edfico y N\u00famerico Computational Physics with Python Matplotlib - gr\u00e1ficos 2D y 3D en Python NumPy Tutorial: Data Analysis with Python Python & OpenGL for Scientific Visualization Videos Python para ciet\u00edficos e ingenieros An\u00e1lisis de Datos Data analysis in Python with pandas The Ancient Art of the Numerati Intro to pandas data structures Analyzing Tweets with Pandas and Matplotlib awesome-etl Useful Pandas Snippets Data Mining in Python Diving into Pandas is Faster than Reinventing it First Steps With PySpark and Big Data Processing Cubes Machine Learnig Learning scikit-learn Detecci\u00f3n Facial face_recognition face-recognition-server Face Detection in Python Using a Webcam Face Recognition using Python and OpenCV Face recognition using OpenCV and Python: A beginner's guide Face Recognition Using OpenCV | Loading Recognizer Real-time Human Detection in Computer Vision Pedestrian Detection OpenCV Face Detection with Python using OpenCV Object Detection: Face Detection using Haar Cascade Classfiers Creating a face detection API with Python and OpenCV Reconocimiento de Voz The Ultimate Guide To Speech Recognition With Python Python speech to text with PocketSphinx Learn how to Build your own Speech-to-Text Model Watson Developer Cloud Python SDK PyQt5 Text To Speech GUI Based Reconocimiento de voz (tiempo real) en idioma espa\u00f1ol con PocketSphinx Procesamiento de Im\u00e1genes Image Processing 101 How to use deep learning for data extraction from financial document A comprehensive guide to OCR with Tesseract, OpenCV and Python Image Difference with OpenCV and Python Basic Image Data Analysis Using Python: Part 1 Basic Image Data Analysis Using Python: Part 2 Detecting machine-readable zones in passport images Mask R-CNN for Ship Detection & Segmentation Image Segmentation with Python Barcode and QR code Scanner using ZBar and OpenCV Image Alignment (Feature Based) using OpenCV (C++/Python) Improve Accuracy of OCR using Image Preprocessing A Step-by-Step Introduction to the Basic Object Detection Algorithms Computer Vision Tutorial: Implementing Mask R-CNN for Image Segmentation Counting blue and white bacteria colonies with Python and OpenCV Using Tesseract OCR with Python Extract Captcha Text using CNN in Python Librerias ImageAI Imagehash fast-style-transfer neural-style Python Tesseract Video Image Processing for Python Procesamiento de V\u00eddeo Extracting and Saving Video Frames import cv2 vidcap = cv2.VideoCapture('big_buck_bunny_720p_5mb.mp4') success, image = vidcap.read() count = 0 while success: cv2.imwrite(\"frame%d.jpg\" % count, image) # save frame as JPEG file success,image = vidcap.read() print('Read a new frame: ', success) count += 1 Start video read from a particular frame f = # put here the frame from which you want to start self.capture.set(CV_CAP_PROP_POS_FRAMES, f) while f < self.frame_count: retval, image = self.capture.read() f += 1 Object Detection Tutorial in TensorFlow: Real-Time Object Detection YOLO Object Detection with OpenCV and Python Convert Video to Images (Frames) & Images (Frames) to Video using OpenCV (Python) Convert Image Frames to Video File using OpenCV in Python PySceneDetect Django Video Encoding Video, Audio Transmuxing into HLS using FFmpeg HLS Muxer in Django What is the Python FFmpeg Video Streaming?","title":"Python"},{"location":"python/#python","text":"","title":"Python"},{"location":"python/#tips-chips","text":"Run a function at the start of every round 5 minute interval from datetime import datetime, timedelta from threading import Timer def get_sleep_time(): now = datetime.now() next_run = now.replace(minute=int(now.minute / 5) * 5, second=0, microsecond=0) + timedelta(minutes=5) return (next_run - now).total_seconds() def dowork(): now = datetime.now() print('Doing some work at', now) schedule_next_run() def schedule_next_run(): sleep_time = get_sleep_time() print(f'sleeping for {sleep_time} seconds') t = Timer(sleep_time, dowork) t.daemon = True t.start() print('Starting work schedule') schedule_next_run() input('Doing work every 5 minutes. Press enter to exit') Managing Multiple Python Versions With Pyenv Performance Tips Good logging practice in Python 30 Essential Python Tips and Tricks for Programmers 30 Python Language Features and Tricks You May Not Know About 9 obscure Python libraries for data science Speed Up Your Python Program With Concurrency The ever useful and neat subprocess module Production-ready Docker packaging Easy parallel HTTP requests with Python and asyncio IP Geolocation API 13 Project Ideas for Intermediate Python Developers A Simple Approach To Templated SQL Queries In Python Twitter Sentiment Analysis Twitter Python Developing and Testing an Asynchronous API with FastAPI and Pytest fbs tutorial Get only NEW Emails imaplib and python","title":"Tips &amp; Chips"},{"location":"python/#web","text":"","title":"Web"},{"location":"python/#django","text":"Django REST with React Django Dashboard CoreUI","title":"Django"},{"location":"python/#flask","text":"Flask Dashboard - CoreUI Design","title":"Flask"},{"location":"python/#gui","text":"","title":"GUI"},{"location":"python/#pyside2","text":"Qt for Python Documentation Qt for Python Tutorials PySide2 examples Pyqtgraph Building a Matplotlib GUI with Qt Designer How to Install PyQt5 and Build Your First GUI in Python 3.4","title":"PySide2"},{"location":"python/#kivy","text":"Kivy VideoPlayer not working: Error loading Texture #6672 pip install pillow ffpyplayer How To Make Simple Mobile Applications Using Kivy In Python? Netclone Netflix UI Clone Audio and Video Streaming issue on Android","title":"Kivy"},{"location":"python/#databases","text":"Writing Data to InfluxDB with Python Playing with Postgres NOTIFY/LISTEN using Python asyncio and psycopg2 Fastest Way to Load Data Into PostgreSQL Using Python","title":"Databases"},{"location":"python/#async-tasks","text":"","title":"Async Tasks"},{"location":"python/#huey","text":"Huey - a nice celery alternative","title":"Huey"},{"location":"python/#celery","text":"","title":"Celery"},{"location":"python/#with-flask","text":"from flask import Flask from celery import Celery app = Flask(__name__) app.config['CELERY_BROKER_URL'] = 'redis://localhost:6379' app.config['CELERY_RESULT_BACKEND'] = 'redis://localhost:6379' celery = Celery(app.name, broker=app.config['CELERY_BROKER_URL']) celery.conf.update(app.config) @celery.task() def some_task(): # ... return {\"result\": \"success\"} @app.route('/some-route') def encoder(): task = some_task.delay() # ... return {\"task_id\": task.id} if __name__ == \"__main__\": app.run(debug=True) Django & Celery \u2013 Easy async task processing Asynchronous Tasks using Celery with Django Setting up an asynchronous task queue for Django using Celery and Redis Asynchronous Tasks With Django and Celery Don't keep important data in your Celery queue How to keep Celery running with supervisor Storing a task id for each celery task in database Celery Exception Handling Running Asynchronous background Tasks on Linux with Python 3 Flask and Celery","title":"With Flask"},{"location":"python/#cientifico-y-numerico","text":"Computational Physics with Python Matplotlib - gr\u00e1ficos 2D y 3D en Python NumPy Tutorial: Data Analysis with Python Python & OpenGL for Scientific Visualization","title":"Cient\u00edfico y N\u00famerico"},{"location":"python/#videos","text":"Python para ciet\u00edficos e ingenieros","title":"Videos"},{"location":"python/#analisis-de-datos","text":"Data analysis in Python with pandas The Ancient Art of the Numerati Intro to pandas data structures Analyzing Tweets with Pandas and Matplotlib awesome-etl Useful Pandas Snippets Data Mining in Python Diving into Pandas is Faster than Reinventing it First Steps With PySpark and Big Data Processing Cubes","title":"An\u00e1lisis de Datos"},{"location":"python/#machine-learnig","text":"Learning scikit-learn","title":"Machine Learnig"},{"location":"python/#deteccion-facial","text":"face_recognition face-recognition-server Face Detection in Python Using a Webcam Face Recognition using Python and OpenCV Face recognition using OpenCV and Python: A beginner's guide Face Recognition Using OpenCV | Loading Recognizer Real-time Human Detection in Computer Vision Pedestrian Detection OpenCV Face Detection with Python using OpenCV Object Detection: Face Detection using Haar Cascade Classfiers Creating a face detection API with Python and OpenCV","title":"Detecci\u00f3n Facial"},{"location":"python/#reconocimiento-de-voz","text":"The Ultimate Guide To Speech Recognition With Python Python speech to text with PocketSphinx Learn how to Build your own Speech-to-Text Model Watson Developer Cloud Python SDK PyQt5 Text To Speech GUI Based Reconocimiento de voz (tiempo real) en idioma espa\u00f1ol con PocketSphinx","title":"Reconocimiento de Voz"},{"location":"python/#procesamiento-de-imagenes","text":"Image Processing 101 How to use deep learning for data extraction from financial document A comprehensive guide to OCR with Tesseract, OpenCV and Python Image Difference with OpenCV and Python Basic Image Data Analysis Using Python: Part 1 Basic Image Data Analysis Using Python: Part 2 Detecting machine-readable zones in passport images Mask R-CNN for Ship Detection & Segmentation Image Segmentation with Python Barcode and QR code Scanner using ZBar and OpenCV Image Alignment (Feature Based) using OpenCV (C++/Python) Improve Accuracy of OCR using Image Preprocessing A Step-by-Step Introduction to the Basic Object Detection Algorithms Computer Vision Tutorial: Implementing Mask R-CNN for Image Segmentation Counting blue and white bacteria colonies with Python and OpenCV Using Tesseract OCR with Python Extract Captcha Text using CNN in Python","title":"Procesamiento de Im\u00e1genes"},{"location":"python/#librerias","text":"ImageAI Imagehash fast-style-transfer neural-style Python Tesseract","title":"Librerias"},{"location":"python/#video","text":"Image Processing for Python","title":"Video"},{"location":"python/#procesamiento-de-video","text":"Extracting and Saving Video Frames import cv2 vidcap = cv2.VideoCapture('big_buck_bunny_720p_5mb.mp4') success, image = vidcap.read() count = 0 while success: cv2.imwrite(\"frame%d.jpg\" % count, image) # save frame as JPEG file success,image = vidcap.read() print('Read a new frame: ', success) count += 1 Start video read from a particular frame f = # put here the frame from which you want to start self.capture.set(CV_CAP_PROP_POS_FRAMES, f) while f < self.frame_count: retval, image = self.capture.read() f += 1 Object Detection Tutorial in TensorFlow: Real-Time Object Detection YOLO Object Detection with OpenCV and Python Convert Video to Images (Frames) & Images (Frames) to Video using OpenCV (Python) Convert Image Frames to Video File using OpenCV in Python PySceneDetect Django Video Encoding Video, Audio Transmuxing into HLS using FFmpeg HLS Muxer in Django What is the Python FFmpeg Video Streaming?","title":"Procesamiento de V\u00eddeo"},{"location":"react/","text":"Instalacion de Dependencias $ yarn add parcel-bundler -D $ yarn add react react-dom react-router-dom $ yarn add regenerator-runtime -D","title":"React"},{"location":"react/#instalacion-de-dependencias","text":"$ yarn add parcel-bundler -D $ yarn add react react-dom react-router-dom $ yarn add regenerator-runtime -D","title":"Instalacion de Dependencias"},{"location":"timescaledb/","text":"TimescaleDB snippets TimescaleDB TimescaleDB: An open-source database built for analyzing time-series data with the power and convenience of SQL \u2014 on premise, at the edge or in the cloud. docker run -d --name timescaledb -p 5432:5432 -e POSTGRES_PASSWORD=password timescale/timescaledb:latest-pg9.6 docker exec -it timescaledb psql -U postgres ./outflux migrate telegraf cpu mem system disk diskio --input-server=http://10.10.10.2:8086 --schema-strategy=DropAndCreate --tags-as-json --output-conn=\"dbname=monitoreo user=admin password=admin2019.* host=172.17.0.2\"","title":"TimescaleDB"},{"location":"timescaledb/#timescaledb-snippets","text":"","title":"TimescaleDB snippets"},{"location":"timescaledb/#timescaledb","text":"TimescaleDB: An open-source database built for analyzing time-series data with the power and convenience of SQL \u2014 on premise, at the edge or in the cloud. docker run -d --name timescaledb -p 5432:5432 -e POSTGRES_PASSWORD=password timescale/timescaledb:latest-pg9.6 docker exec -it timescaledb psql -U postgres ./outflux migrate telegraf cpu mem system disk diskio --input-server=http://10.10.10.2:8086 --schema-strategy=DropAndCreate --tags-as-json --output-conn=\"dbname=monitoreo user=admin password=admin2019.* host=172.17.0.2\"","title":"TimescaleDB"}]}